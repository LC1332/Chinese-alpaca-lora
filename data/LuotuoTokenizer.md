# 骆驼Tokenizer的计划

## 动机

+ 原来的LLaMA中文支持能力差

+ 并入中文的tokenizer和原空间不对齐，重训大量知识会混乱

+ 我们希望有一个中文支持更好的tokenizer，还是和原来的LLaMA对齐


## 目标

我们希望我们的Tokenizer有下面这些特征

+ 首先是空间对齐，比如"铁"这个字，和英语的"iron"能够在LLaMA原来的空间上对齐

+ 能够使用四角编码/五笔编码，泛化到任意的汉字

+ 高频的汉字能有独立的token，并且根据其高频的一些词汇，确定在LLaMA上对应的向量

+ 低频的汉字能够组织成四角编码/五笔编码，并且偏旁部首也获得合理的embedding。比如金字旁就会来源于铜、铁等字的平均，这样当模型面对锎、镧这些低频字的时候，就会有办法处理。

+ 我们希望有两个token，一个在llama的基础上只增不减，一个还是删除一些英语的token，使得colab普通机器也可以载入这个tokenizer

GLM和Chinese Alpaca > 15G

## 四角/五笔

铜 87720 qmgk  <heng1-87><heng2-72><heng3-0>
铁 85780 qrwy  <heng1-87><heng2-58><heng3-0>
铝 86760 qkkg  <heng1-87><heng2-66><heng3-0>

苹 44409 aguh <zong1-44><zong2-40><zong3-9> 
草 44406 ajj
苦 44604 adf


## 可能使用到的工具

+ 四角编码/五笔编码

+ 中英文翻译 （这个我们可以去翻译，可以参考github上的新华字典项目）

+ OpenAI Embedding 4096维


## 要注意到的问题

+ 要注意

## 检验方法

分析重点word embedding层，下一层

+ 比较的是 原模型 跑英语 —————— 新模型 跑中文

+ 比low level的时候，feature的average pooling是不是一样 是一样就很厉害

+ 如果这个成功，adapt到新语言 就很容易了

+ embedding可视化 / 求差 / 余弦。 寻找一个带分类的数据集，准备中英文 MMC4-130k

## 缓慢启动的路径

+ 先试着替换， 找1-2个英文，直接换成 之前没有的中文

+ 先试着删除 1-2个 重复的英文 看看会不会影响。 

+ 试着加一两个中文，并且替换合适的embedding vector

+ 扫文本，把特别低频的token 都给替换成 中文字

+ tokenize的时候能输出成你的id

+ 后面可以考虑搞训练了